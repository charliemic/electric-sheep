# ğŸ§  Test Automation: Human-Like Cognitive Process

---

## ğŸ¯ Test Session Goal

**Task**: Sign up for a new account and add a mood value  
**Persona**: Tech Novice (less comfortable with technology)  
**Approach**: Complete the task using visual perception, adaptive planning, and human-like decision-making

---

## ğŸ“‹ The Journey: Step-by-Step

### **Phase 1: Initial Assessment**
ğŸ‘ï¸ **PERCEPTION**: Extracted 179 characters of text from the landing screen  
ğŸ§  **PLANNING**: Decomposed task â†’ identified goal: `AUTHENTICATE`  
ğŸ§  **PLANNING**: Observed gap: `LANDING_SCREEN` â†’ need `NAVIGATE_TO_AUTHENTICATION`  
ğŸ’¡ **DECISION**: Visual detection found no buttons â†’ fallback to semantic identifiers

### **Phase 2: Navigation**
âœ‹ **ACTION**: Tapped "Mood Management"  
ğŸ’­ *Persona thought: "This looks like the main feature. Let me explore it."*  
âœ… **RESULT**: Successfully navigated to sign-up screen

### **Phase 3: Account Creation**
âœ‹ **ACTION**: Tapped "Show email and password sign in"  
âœ‹ **ACTION**: Typed email: `emily6566@outlook.com`  
ğŸ‘ï¸ **PERCEPTION**: Monitored screen â†’ OCR text increased (179 â†’ 243 chars)  
ğŸ‘ï¸ **PERCEPTION**: Detected keyboard presence via text indicators  
âœ‹ **ACTION**: Typed password  
ğŸ‘ï¸ **PERCEPTION**: Continued monitoring (245 â†’ 253 chars)  
âœ‹ **ACTION**: Tapped "Create Account"  
ğŸ‘ï¸ **PERCEPTION**: Watched for response (257-258 chars, keyboard still visible)  
â³ **WAIT**: System waited for app to process account creation

### **Phase 4: Mood Entry**
âœ‹ **ACTION**: Typed mood score: `6`  
ğŸ‘ï¸ **PERCEPTION**: Observed text change (257 â†’ 269 â†’ 263 chars)  
âœ‹ **ACTION**: Tapped "Save Mood"  
ğŸ‘ï¸ **PERCEPTION**: Monitored final state (234-245 chars)

### **Phase 5: Verification**
âœ… **VERIFICATION**: Checked authentication status  
   - ğŸ‘ï¸ Visually searched for authenticated indicators: "mood management", "mood history"  
   - ğŸ‘ï¸ Confirmed no unauthenticated indicators: "sign in", "login"  
   - âœ… **Result**: Authenticated  
âœ… **VERIFICATION**: Checked mood history  
   - ğŸ‘ï¸ Visually searched for "Mood History" text in screenshot  
   - âœ… **Result**: Found

---

## âœ… Task Completed Successfully

**Outcome**: User authenticated and mood value added (`auth=true`, `mood=true`)  
**Method**: Visual verification confirmed completion by "seeing" the result, not querying internal state

---

## ğŸ§  The Brain: Four Cognitive Modes

| Mode | Function | How It Works |
|------|----------|--------------|
| **ğŸ‘ï¸ PERCEPTION** | "What do I see?" | Uses OCR to extract text from screenshots (179-269 chars), visually detects keyboard, monitors state changes every 500ms |
| **ğŸ§  PLANNING** | "What should I do?" | Decomposes tasks into abstract goals (`AUTHENTICATE`), identifies current state vs. goal state, plans actions to close gaps |
| **âœ‹ ACTION** | "Let me do it" | Executes taps, typing, swipes using semantic identifiers when visual detection fails |
| **âœ… VERIFICATION** | "Did it work?" | Visually confirms success by searching for indicators in screenshots (OCR), not by querying internal app state |

### Key Insight
The system mimics human cognition: it **sees** (perception), **thinks** (planning), **acts** (action), and **verifies** (verification) â€” all through visual observation, just like a human would interact with an app.

---

*Generated from automated test execution log*  
*Demonstrating cognitive automation that mirrors human behavior*

